{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "from collections import Counter\n",
    "import joblib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Bidirectional, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from nltk.corpus import stopwords\n",
    "from parsivar import Normalizer\n",
    "import emoji\n",
    "import emojies\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CleanText:\n",
    "    def __init__(self, data_frame, column_name):\n",
    "        self.cln_list = data_frame[column_name].tolist()\n",
    "    def __new__(cls, data_frame, column_name,*args, **kwargs):\n",
    "        data_frame[column_name] = data_frame[column_name].apply(lambda x: x[:400])\n",
    "        return super().__new__(cls,*args, **kwargs)\n",
    "    def clean_punctual(self):\n",
    "        tmp_lst = list(map(lambda x: re.sub(r'https?:\\S*', ' ', x), self.cln_list))\n",
    "        tmp_lst = list(map(lambda x: re.sub(r'@[A-Za-z0-9]\\S+', ' ', x), tmp_lst))\n",
    "        tmp_lst = list(map(lambda x: re.sub(r'[0-9]\\S+', ' ', x), tmp_lst))\n",
    "        self.cln_list = list(map(lambda x: re.sub(r'#|_|:|/d+', ' ', x), tmp_lst))\n",
    "        return self.cln_list\n",
    "    def normalize_text(self):\n",
    "        normalizer = Normalizer(pinglish_conversion_needed=True)\n",
    "        cln_list = list(map(lambda x: normalizer.normalize(x), self.cln_list))\n",
    "        self.cln_list = list(map(lambda x: ''.join(ch for ch, _ in itertools.groupby(x)), cln_list))\n",
    "        return self.cln_list\n",
    "    def remove_stop_words(self):\n",
    "        stop_words = set(stopwords.words('RD_persian_01'))\n",
    "        self.cln_list = list(map(lambda x: ' '.join([w for w in x.split() if not w in stop_words]), self.cln_list))\n",
    "        return self.cln_list\n",
    "    def extract_emojis(self):\n",
    "        self.cln_list = list(map(lambda x: ''.join((' '+c+' ') if c in emoji.UNICODE_EMOJI['en'] else c for c in x), self.cln_list))\n",
    "        return self.cln_list\n",
    "    def convert_emojies(self):\n",
    "        self.cln_list = list(map(lambda x: emojies.replace(x), self.cln_list))\n",
    "        return self.cln_list\n",
    "    def frequency_words(self):\n",
    "        freq = dict(Counter(\" \".join(self.cln_list).split()))\n",
    "        sort_orders = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        sort_orders = sort_orders[:4000]\n",
    "        # print(sort_orders)\n",
    "        print(len(sort_orders))\n",
    "        most_common_word = [i[0] for i in sort_orders]\n",
    "        most_common_word = set(most_common_word)\n",
    "        # print(most_common_word)\n",
    "        print(len(most_common_word))\n",
    "        self.cln_list = list(map(lambda x: ' '.join([w for w in x.split() if w in most_common_word]), self.cln_list))\n",
    "        return self.cln_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EncodeText:\n",
    "    def __init__(self,train_text):\n",
    "        self.train_text = train_text\n",
    "    def create_tokenizer(self):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.train_text)\n",
    "        return tokenizer\n",
    "    def encode_text(self,tokenizer, input_list, max_length):\n",
    "        # integer encode\n",
    "        encoded = tokenizer.texts_to_sequences(input_list)\n",
    "        # pad encoded sequences\n",
    "        padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "        return padded\n",
    "    def label_encoder(self, tag):\n",
    "        le = LabelEncoder()\n",
    "        tmp_tag = le.fit_transform(tag)\n",
    "        encode_tag = to_categorical(np.array(tmp_tag))\n",
    "        return encode_tag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data_df = pd.read_excel('dataset.2.0.0.xlsx', index_col= False)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.hist(data_df['احساس'])\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('comp_dataset2.csv', index_col= False)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_df['tag'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# call_cleantext = CleanText(data_df, 'متن توییت')\n",
    "# call_cleantext = CleanText(data_df, 'Text')\n",
    "call_cleantext = CleanText(data_df, 'caption')\n",
    "get_pun_list = call_cleantext.clean_punctual()\n",
    "print(get_pun_list[64])\n",
    "get_ex_emoji = call_cleantext.extract_emojis()\n",
    "print(get_ex_emoji[64])\n",
    "get_emoji_list = call_cleantext.convert_emojies()\n",
    "print(get_emoji_list[64])\n",
    "get_norm_list = call_cleantext.normalize_text()\n",
    "print(get_norm_list[64])\n",
    "get_rm_sw_list = call_cleantext.remove_stop_words()\n",
    "print(get_rm_sw_list[64])\n",
    "get_most_com_list = call_cleantext.frequency_words()\n",
    "print(get_most_com_list[64])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_most_com_list = call_cleantext.frequency_words()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# init_List_prepared = list(map(lambda x: [x[0], x[1]], zip(get_most_com_list, data_df['احساس'])))\n",
    "init_List_prepared = list(map(lambda x: [x[0], x[1]], zip(get_most_com_list, data_df['tag'])))\n",
    "init_List_prepared = list(filter(lambda x: len(x[0])>1, init_List_prepared))\n",
    "random.shuffle(init_List_prepared)\n",
    "\n",
    "var1 = list(filter(lambda x: x[1]=='شادی', init_List_prepared))\n",
    "var2 = list(filter(lambda x: x[1]=='خشم', init_List_prepared))\n",
    "var3 = list(filter(lambda x: x[1]=='غم', init_List_prepared))\n",
    "var4 = list(filter(lambda x: x[1]=='خنثی', init_List_prepared))\n",
    "var5 = list(filter(lambda x: x[1]=='امید', init_List_prepared))\n",
    "var6 = list(filter(lambda x: x[1]=='ترس', init_List_prepared))\n",
    "var7 = list(filter(lambda x: x[1]=='تعجب', init_List_prepared))\n",
    "var8 = list(filter(lambda x: x[1]=='تحسین یا اعتماد', init_List_prepared))\n",
    "\n",
    "# List_prepared = var1[:1500]+var2[:1500]+var3[:1000]+var4[:1000]+var5[:1200]+var6[:700]+var7[:500]+var8[:1500]\n",
    "# List_prepared = var1[:500]+var2[:500]+var3[:500]+var4[:500]+var5[:500]+var6[:500]+var7[:500]+var8[:500]\n",
    "# List_prepared = var1[:2700]+var2[:2700]\n",
    "# List_prepared = var1+var2+var5+var8\n",
    "List_prepared = var1[:2000]+var2\n",
    "\n",
    "# List_prepared = init_List_prepared"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text, tag = zip(*List_prepared)\n",
    "train_text, test_text, train_tag, test_tag = train_test_split(text, tag, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% train & test split\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "call_encodetext = EncodeText(train_text)\n",
    "tokenizer = call_encodetext.create_tokenizer()\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "max_len = 100\n",
    "encode_train_text = call_encodetext.encode_text(tokenizer, train_text, max_len)\n",
    "encode_test_text = call_encodetext.encode_text(tokenizer, test_text, max_len)\n",
    "encode_train_tag = call_encodetext.label_encoder(train_tag)\n",
    "encode_test_tag = call_encodetext.label_encoder(test_tag)\n",
    "num_cat = encode_train_tag.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_len))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.3)))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(num_cat, activation='softmax'))\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fitting Network\n",
    "m = model.fit(encode_train_text, encode_train_tag, epochs=50, verbose=2)\n",
    "# Evaluating Network\n",
    "loss, acc = model.evaluate(encode_test_text, encode_test_tag, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc * 100))\n",
    "print('Test loss: %f' % loss)\n",
    "# !khashm, khonsa, shadi, gham of 1e4 51%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with open('CNN_BiLSTM_6_tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # # loading\n",
    "# # with open('CNN_BiLSTM_6_tokenizer.pickle', 'rb') as handle:\n",
    "# #     tokenizer = pickle.load(handle)\n",
    "# joblib.dump(model, 'CNN_BiLSTM_6_model.joblib')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% save model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test_df = pd.DataFrame()\n",
    "# test_df['train_text'] = train_text\n",
    "# test_df['tag_text'] = train_tag\n",
    "# test_df['encode_train_tag'] = encode_train_tag.tolist()\n",
    "# test_df.to_excel('test2_df.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Evaluate test file\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tmp_df = pd.DataFrame()\n",
    "# comment = train_text\n",
    "# tmp_df['tmp_caption']=comment\n",
    "# print(tmp_df)\n",
    "# tmp_call_cleantext = CleanText(tmp_df, 'tmp_caption')\n",
    "# tmp_get_ex_emoji = tmp_call_cleantext.extract_emojis()\n",
    "# print(tmp_get_ex_emoji)\n",
    "# tmp_get_emoji_list = tmp_call_cleantext.convert_emojies()\n",
    "# tmp_get_norm_list = tmp_call_cleantext.normalize_text()\n",
    "# print(tmp_get_norm_list)\n",
    "#\n",
    "# list_a = []\n",
    "# i = 0\n",
    "# for item in tmp_get_norm_list:\n",
    "#     i += 1\n",
    "#     # print(i)\n",
    "#     comment_list = item\n",
    "#     instance = tokenizer.texts_to_sequences(comment_list)\n",
    "#     flat_list = []\n",
    "#     for sublist in instance:\n",
    "#         for item in sublist:\n",
    "#             flat_list.append(item)\n",
    "#\n",
    "#     flat_list = [flat_list]\n",
    "#\n",
    "#     instance = pad_sequences(flat_list, padding='post', maxlen=max_len)\n",
    "#\n",
    "#     output = model.predict(instance)\n",
    "#     list_a.append(output)\n",
    "# # print(list_a)\n",
    "# train_df = pd.DataFrame()\n",
    "# train_df['train_text'] = train_text\n",
    "# train_df['tag_text'] = train_tag\n",
    "# train_df['encode_train_tag'] = encode_train_tag.tolist()\n",
    "# train_df['my_model'] = list_a\n",
    "# train_df.to_excel('train2_df.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Evaluate train file\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = pd.read_excel('train2_df.xlsx')\n",
    "# list_b = df['my_model']\n",
    "# list_c = list(map(lambda x: list(x[2:-2].split(\" \")), list_b))\n",
    "# list_c = list(map(lambda x: list(filter(None, x)), list_c))\n",
    "#\n",
    "# list_d = list(map(lambda x: [re.sub('\\n','', i) for i in x], list_c))\n",
    "#\n",
    "# list_e = list(map(lambda x: [float(i) for i in x], list_d))\n",
    "# list_f = list(map(lambda x: x.index(max(x)), list_e))\n",
    "# train_df['my_model2'] = list_f\n",
    "# train_df.to_excel('train2_df.xlsx', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}