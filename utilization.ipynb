{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "import random\n",
    "from collections import Counter\n",
    "import joblib\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras import Sequential, optimizers\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Bidirectional, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from nltk.corpus import stopwords\n",
    "from parsivar import Normalizer\n",
    "import emoji\n",
    "import emojies\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class CleanText:\n",
    "    def __init__(self, data_frame, column_name):\n",
    "        self.cln_list = data_frame[column_name].tolist()\n",
    "    def __new__(cls, data_frame, column_name,*args, **kwargs):\n",
    "        data_frame[column_name] = data_frame[column_name].apply(lambda x: x[:400])\n",
    "        return super().__new__(cls,*args, **kwargs)\n",
    "    def clean_punctual(self):\n",
    "        tmp_lst = list(map(lambda x: re.sub(r'https?:\\S*', ' ', x), self.cln_list))\n",
    "        tmp_lst = list(map(lambda x: re.sub(r'@[A-Za-z0-9]\\S+', ' ', x), tmp_lst))\n",
    "        tmp_lst = list(map(lambda x: re.sub(r'[0-9]\\S+', ' ', x), tmp_lst))\n",
    "        self.cln_list = list(map(lambda x: re.sub(r'#|_|:|/d+', ' ', x), tmp_lst))\n",
    "        return self.cln_list\n",
    "    def normalize_text(self):\n",
    "        normalizer = Normalizer(pinglish_conversion_needed=True)\n",
    "        cln_list = list(map(lambda x: normalizer.normalize(x), self.cln_list))\n",
    "        self.cln_list = list(map(lambda x: ''.join(ch for ch, _ in itertools.groupby(x)), cln_list))\n",
    "        return self.cln_list\n",
    "    def remove_stop_words(self):\n",
    "        stop_words = set(stopwords.words('RD_persian_01'))\n",
    "        self.cln_list = list(map(lambda x: ' '.join([w for w in x.split() if not w in stop_words]), self.cln_list))\n",
    "        return self.cln_list\n",
    "    def extract_emojis(self):\n",
    "        self.cln_list = list(map(lambda x: ''.join((' '+c+' ') if c in emoji.UNICODE_EMOJI['en'] else c for c in x), self.cln_list))\n",
    "        return self.cln_list\n",
    "    def convert_emojies(self):\n",
    "        self.cln_list = list(map(lambda x: emojies.replace(x), self.cln_list))\n",
    "        return self.cln_list\n",
    "    def frequency_words(self):\n",
    "        freq = dict(Counter(\" \".join(self.cln_list).split()))\n",
    "        sort_orders = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        sort_orders = sort_orders[:4000]\n",
    "        # print(sort_orders)\n",
    "        print(len(sort_orders))\n",
    "        most_common_word = [i[0] for i in sort_orders]\n",
    "        most_common_word = set(most_common_word)\n",
    "        print(most_common_word)\n",
    "        # print(len(most_common_word))\n",
    "        self.cln_list = list(map(lambda x: ' '.join([w for w in x.split() if w in most_common_word]), self.cln_list))\n",
    "        return self.cln_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class EncodeText:\n",
    "    def __init__(self,train_text):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_text(self,input_list, max_length):\n",
    "        # integer encode\n",
    "        encoded = self.tokenizer.texts_to_sequences(input_list)\n",
    "        # pad encoded sequences\n",
    "        padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "        return padded\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "data_df = pd.read_excel('posnegtest.xlsx', index_col=False)\n",
    "# Load tokenizer\n",
    "with open(r'F:/sourcecode/sentiment_analysis_01/model/2_cat_softmax_posneg/cnn+bilstm/CNN_BiLSTM_6_tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "# Load model\n",
    "filename = r'\\sourcecode\\sentiment_analysis_01\\model\\2_cat_softmax_posneg\\cnn+bilstm\\CNN_BiLSTM_6_model.h5'\n",
    "model = keras.models.load_model(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Read excel file\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "tmp_call_cleantext = CleanText(data_df, 'caption')\n",
    "tmp_get_ex_emoji = tmp_call_cleantext.extract_emojis()\n",
    "tmp_get_emoji_list = tmp_call_cleantext.convert_emojies()\n",
    "tmp_get_norm_list = tmp_call_cleantext.normalize_text()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Clean data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "list_a = []\n",
    "max_len = 100\n",
    "\n",
    "call_encodetext = EncodeText(tokenizer)\n",
    "encode_text = call_encodetext.encode_text(tmp_get_norm_list, max_len)\n",
    "# print(encode_text)\n",
    "\n",
    "for item in encode_text:\n",
    "    item = item.tolist()\n",
    "    item = [item]\n",
    "    output = model.predict(item)\n",
    "    list_a.append(output)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "result_df['text'] = data_df['caption'].copy()\n",
    "result_df['my_model'] = list_a\n",
    "result_df.to_excel('result_df.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "abs_df = pd.read_excel('result_df.xlsx')\n",
    "list_b = abs_df['my_model']\n",
    "list_c = list(map(lambda x: list(x[2:-2].split(\" \")), list_b))\n",
    "list_c = list(map(lambda x: list(filter(None, x)), list_c))\n",
    "\n",
    "list_d = list(map(lambda x: [re.sub('\\n','', i) for i in x], list_c))\n",
    "\n",
    "list_e = list(map(lambda x: [float(i) for i in x], list_d))\n",
    "list_f = list(map(lambda x: x.index(max(x)), list_e))\n",
    "result_df['split_my_model'] = list_f\n",
    "result_df.to_excel('result_df.xlsx', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}